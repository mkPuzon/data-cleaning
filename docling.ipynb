{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b532b4",
   "metadata": {},
   "source": [
    "# Docling Class Structures and Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcd4f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' General Utility Functions '''\n",
    "def get_obj_methods(obj: object) -> list[str]:\n",
    "    return [func for func in dir(obj) if not func.startswith(\"__\") and callable(getattr(obj, func))]\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DOC_SOURCE = \"data/TestPDF.pdf\"\n",
    "output_dir = Path(\"scratch\")\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477d33fc",
   "metadata": {},
   "source": [
    "## Docling Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b4ce79",
   "metadata": {},
   "source": [
    "### DoclingDocument - [Official docs](https://docling-project.github.io/docling/reference/docling_document/#docling_core.types.doc.DoclingDocument)\n",
    "<class 'docling_core.types.doc.document.DoclingDocument'>\n",
    "\n",
    "**Attributes**: ['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages']\n",
    "\n",
    "**Notable Methods**: ['export_to_dict', 'export_to_doctags', 'export_to_document_tokens', 'export_to_element_tree', 'export_to_html', 'export_to_markdown', 'export_to_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d9d54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-21 19:59:25,343 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-21 19:59:25,642 - INFO - Going to convert document batch...\n",
      "2025-12-21 19:59:25,645 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-21 19:59:25,656 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-21 19:59:25,688 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-21 19:59:25,691 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-21 19:59:25,993 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 19:59:25,996 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 19:59:26,672 [RapidOCR] download_file.py:60: File exists and is valid: /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 19:59:26,674 [RapidOCR] main.py:50: Using /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 19:59:28,599 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 19:59:28,602 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 19:59:28,681 [RapidOCR] download_file.py:60: File exists and is valid: /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 19:59:28,684 [RapidOCR] main.py:50: Using /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 19:59:29,309 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 19:59:29,310 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 19:59:29,896 [RapidOCR] download_file.py:60: File exists and is valid: /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 19:59:29,899 [RapidOCR] main.py:50: Using /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-21 19:59:31,231 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-21 19:59:31,233 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-21 19:59:33,086 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-21 19:59:34,221 - INFO - Processing document TestPDF.pdf\n",
      "\u001b[33m[WARNING] 2025-12-21 19:59:44,595 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2025-12-21 19:59:44,598 - WARNING - RapidOCR returned empty result!\n",
      "\u001b[33m[WARNING] 2025-12-21 19:59:55,769 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2025-12-21 19:59:55,771 - WARNING - RapidOCR returned empty result!\n",
      "\u001b[33m[WARNING] 2025-12-21 19:59:57,995 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2025-12-21 19:59:57,997 - WARNING - RapidOCR returned empty result!\n",
      "\u001b[33m[WARNING] 2025-12-21 20:00:00,087 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2025-12-21 20:00:00,091 - WARNING - RapidOCR returned empty result!\n",
      "2025-12-21 20:00:03,976 - WARNING - RapidOCR returned empty result!\n",
      "2025-12-21 20:00:17,649 - INFO - Finished converting document TestPDF.pdf in 52.34 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Optic flow is used to control human walking\n",
      "\n",
      "William H. Warren, Jr., Bruce A. Kay, Wendy D. Zosh, Andrew P. Duchon and Stephanie Sahuc\n",
      "\n",
      "Department of Cognitive and Linguistic Sciences, Brown University, Providence, Rhode Island 02912, USA Correspondence should be addressed to W.W. (bill\\_warren@brown.edu)\n",
      "\n",
      "How is human locomotion visually controlled? Fifty years ago, it was proposed that we steer to a goal using optic flow, the pattern of motion at the eye that specifies the direction of locomotion. However, we might also simply walk in the perceived direction of a goal. These two hypotheses normally predict the same behavior, but we tested them in an immersive virtual environment by displacing the optic flow from the direction of walking, violating the laws of optics. We found that people walked in the visual direction of a lone target, but increasingly relied on optic flow as it was added to the display. The visual control law for steering toward a goal is a linear combination of these two variables weighted by the magnitude of flow, thereby allowing humans to have robust locomotor control under varying environmental conditions.\n",
      "\n",
      "The visual control of locomotion has been a matter of speculation for fifty years. T o arrive at a goal, an obvious solution is simply to move in the perceived direction of the goal. However, James Gibson proposed that steering is based on optic flow, the pattern of visual motion at the moving eye 1 (for example, Fig. 1 ). When an observer travels on a straight path, a radial pattern of optic flow is produced with a focus of expansion (FOE) in the current direction of locomotion, or heading 2 . Gibson proposed that one could steer to a goal by 'keeping the focus of expansion in the direction one must go 1 .' Indeed, humans judge their heading from optic flow with an accuracy better than 1° of visual angle 3-5 , even during pursuit eye movements 6-9 . The neural support for heading perception may be provided by cells in primate cortical areas MSTd, 7a and STPa that are selective for large-field flow patterns 10-15 and by homologous areas in human cortex (H. Peuskens et al. Soc. Neurosci. Abstr. , 25 , 618, 1999) 16,17 . T aken together, these findings suggest that optic flow could be used to control locomotion. However, researchers have primarily relied on psychophysical observations and have not determined whether optic flow is actually used in locomotor tasks 18 . Some have argued that optic flow does not have a significant role in human locomotion 19-21 .\n",
      "\n",
      "Here we tested whether people actually use optic flow to walk toward a goal. Specifically, the optic flow hypothesis states that the observer moves so as to cancel the error between the heading perceived from optic flow and the goal, effectively placing the FOE on the target. Subsequent flow provides information ('reafference') about the adequacy of the steering adjustment, in a perception-action loop. This is similar to a 'tracking' strategy in vehicular navigation. In contrast, the egocentric direction hypothesis states that the observer perceives the visual direction of the goal with respect to the body, and walks in that direction. This might be accomplished by centering the goal at the midline and moving forward as observed in the hoverfly 22 , similar to a 'homing' strategy in navigation.\n",
      "\n",
      "The two hypotheses usually are redundant and predict the same behavior. To dissociate them, we used an immersive virtual environment that permitted us to simulate violations of the laws of optics. Subjects walked freely in a 12 meter × 12 meter room while wearing a stereoscopic head-mounted display (HMD) with a\n",
      "\n",
      "60° H × 40° V field of view. Head position was measured with a tracking system and was used to update the display in real time, as well as to record the subject's path through the virtual world. In the critical manipulation ( Fig. 1 ), the heading direction specified by optic flow was displaced by an angle of δ = 10° from the actual direction of walking, randomly to the right or left on each trial. Thus, if the subject walked in the egocentric direction of the goal, the FOE would appear to the right (or left) of the goal, yielding a virtual heading error of α = 10°. If the subject walked placing the FOE on the goal, the virtual heading error would be α = 0°, but the subject would actually walk slightly to the left (or right) of the goal. Over time, the egocentric direction hypothesis predicts a curved path in both the physical and virtual worlds and a virtual heading error of 10° ( Fig. 2a ), whereas the optic flow hypothesis predicts a straight path and a heading error that goes to zero ( Fig. 2b ). A combination of the two would yield an intermediate trajectory.\n",
      "\n",
      "## RESULTS\n",
      "\n",
      "We asked subjects to walk to a goal nine meters away, in four virtual worlds that varied the available optic flow. In the first virtual world, a thin target line ran off the top and bottom of the display ( Fig. 3a ), and no surrounding flow was defined. Subjects followed a curved path close to that predicted by the egocentric direction hypothesis, and maintained a virtual heading error close to 10°. This confirmed that people rely on egocentric direction when no flow is available. (The reduction in error near the end of the trial is likely due to the late expansion of the target line.) We also observed that subjects tend to face the goal and walk forward, consistent with a centering strategy. The second virtual world added a textured ground plane to provide some optic flow ( Fig. 3b ), and subjects in this case took an intermediate curved path. Subjects initially walked in the egocentric direction of the target line (heading error near 10°), but over the first two to three meters, they reduced this error to about 5°, revealing an influence of the ground flow. In the third virtual world, subjects walked through a doorway with a textured floor and ceiling ( Fig. 3c ), and had even straighter paths and smaller heading errors. In the fourth, we added an array of textured posts to the doorway display ( Fig. 3d ) to create a cluttered scene with high motion paral- articles\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## articles\n",
      "\n",
      "lax, a form of optic flow. This produced the straightest path and an overall heading error below 2°, close to the predictions of the optic flow hypothesis. Across the four worlds, the mean lateral deviation of the path decreased significantly ( F 3,27 = 36.81, p &lt; 0.001), as did the mean heading error ( F 3,27 = 30.33, p &lt;0.001). The turning rate in the first two to three meters became faster. (The slope of heading error in Fig. 3 gets steeper.)\n",
      "\n",
      "It thus seems that locomotion was governed by a combination of the two variables. When no flow was available, behavior was consistent with the egocentric direction hypothesis, but as flow was added to the display, it increasingly dominated behavior, following the optic flow hypothesis more consistently. Subjects in the three textured worlds started walking in the egocentric direction of the goal at the beginning of each trial, and then changed course during the first two to three meters (two to three seconds) as the optic flow was detected. This indicates that the reduction in heading error across the four worlds cannot simply be attributed to visual adaptation of perceived target direction. Subjects tended to face the goal and 'crab' slightly sideways, suggesting a shift in the direction of thrust rather than a shift in the perceived straight ahead.\n",
      "\n",
      "Rushton and colleagues 19 examined the same issue by using displacing prisms to offset the optic flow by δ = 16° as subjects walked across a grass lawn. Contrary to our results, they reported that subjects followed curved paths with heading errors close to 16°, as if guided solely by egocentric direction. This finding has been interpreted to support the view that optic flow has no significant importance in the control of locomotion 20,21 . However, prisms introduce optical distortion that warps the flow pattern, which might lead subjects to depend more on egocentric direction. We tested this in a second experiment, by having subjects wear binocular wedge prisms inside our HMD while viewing the same four worlds. The prisms displaced the image of the virtual environment by δ = 10°, always to the right. This was compared with a no-prism control in which the computed offset in the HMD was always δ = 10° to the right.\n",
      "\n",
      "The results were similar to those in the first experiment, but were attenuated with the prisms. In the control condition ( Fig. 4a ), the lateral deviation of the path and the heading error decreased as flow was added to the display (F 3,33 = 15.26, p &lt; 0.001, and F 3,33 = 15.46, p &lt; 0.001, respectively), replicating the first experiment. In the prism condition ( Fig. 4b ), a similar reduction in lateral deviation and heading error also occurred (F 3,33 = 10.65, p &lt; 0.001, and F 3,33 = 9.41,\n",
      "\n",
      "Fig. 1. Manipulating optic flow in a virtual environment. Locomotion normally produces optic flow with a focus of expansion (FOE) in the direction of walking (T). We displaced the FOE by δ = 10° from the walking direction, and measured the virtual heading error α between the FOE and  the  goal. β represents  the  egocentric  direction  of  the  goal  with respect to the axis of thrust.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "p &lt; 0.001, respectively), but the effect was significantly smaller (world × prism interaction, F 3,33 = 3.00, p &lt; 0.05 and F 3,33 = 3.01, p &lt; 0.05, respectively). This indicates that prismatic distortion leads subjects to rely more on egocentric direction. Nonetheless, the ground environment in the prism condition-which is comparable to Rushton and colleagues' open field-showed a clear influence of the optic flow, with moderately curved paths midway between the two hypotheses ( Fig. 4b , dashed curve). This suggests that our coarse noise texture provided more detectable flow than their fine grass texture. A similar effect occurs in open-field prism experiments when the optic flow is enhanced by adding visual structure on the ground (B.J. Rogers &amp; R.S. Allison, Perception 28 Suppl. , 2, 1999) 23 . Even though our displacement was always to the right, subjects again tended to walk toward the goal at the start of each trial (large initial heading errors in Fig. 4 ), so the subsequent reduction in error cannot be attributed to visual adaptation of perceived direction.\n",
      "\n",
      "Another possible explanation of the straighter paths we observed in textured environments should be considered, an explanation based on the egocentric direction strategy. The offset of the focus\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Fig. 2. Predictions of the two hypotheses, in a physical world coordinate frame. ( a ) Egocentric direction hypothesis: the subject walks from point A to point B, in the direction of the goal at a . However, we displace the subject to point  B´  in  the  virtual  world,  creating  a  virtual  heading δ = 10° to the right of the goal. The goal consequently drifts leftward in the physical world at about 0.1 m/s. On the next time step, the subject walks from point B to point C, toward the new position of the goal at b . Iteratively, this predicts  a  curved  path  in  both  the  physical  and  virtual worlds, and a constant virtual heading error of α = 10°. ( b ) Optic flow hypothesis: the subject walks 10° to the left of the goal at a (from point A to B), placing the FOE on the goal. The virtual heading is thus toward the goal (from A to B´). Goal and observer drift leftward together, such that the relative position between the FOE and the goal is constant. Iteratively, this predicts a straight path in both physical and virtual worlds, and a virtual heading error of α = 0. (T o visualize the paths in the virtual world, one can redraw the diagrams putting the dashed arrows head-to-tail and superimposing the goal in each time-step.)\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "n.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "n.\n",
      "\n",
      "Fig. 3. The four virtual worlds, and results of experiment 1. ( a ) 'T arget line,' ( b ) 'line + ground,' ( c ) 'doorway,' ( d ) 'doorway + posts.' Center column, mean path in the virtual world; right column, mean virtual heading error ( α ) as a function of longitudinal position. Data ( ------) are collapsed and plotted as though the displacement is to the right ( δ = 10°), together with the between-subject standard error (·········). The predictions of the egocentric direction hypothesis (++++) and the optic flow hypothesis ( ) are also indicated.\n",
      "\n",
      "of expansion to the right caused surfaces in front of the observer to drift to the left. This may have been perceived as a slow observer rotation to the right, inducing a rightward drift of the perceived straight ahead. This could have resulted in an illusory leftward drift of the perceived egocentric direction of the target (relative to the perceived straight ahead), leading the observer to walk left of the actual target along a straighter path. However, this explanation rests on several convenient assumptions, and a number of observations render it unlikely. First, despite the local drift of frontal surfaces, the global flow pattern was not consistent with observer rotation, but specified translation along a displaced axis, and the subjective experience was one of translation rather than self-rotation. Second, data from a variety of tasks show that although background flow induces illusory motion of a target, it does not influence responses to the egocentric direction of the target 24-26 . Third, if the perceived egocentric direction of the target drifts, we might expect that subjects would face in the illusory target direction, but they tended to face in the actual target direction. Taken together, these considerations make it unlikely that drift in the perceived straight ahead is a plausible explanation of the present results.\n",
      "\n",
      "## DISCUSSION\n",
      "\n",
      "These findings establish that humans rely on both optic flow and egocentric direction to guide locomotion to a goal. We modeled this pattern of behavior with a visual control law 27,28 , in which the turning rate (d φ /d t ) is a linear sum of egocentric direction\n",
      "\n",
      "## articles\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "and optic flow, the latter weighted by the magnitude of flow in the field of view:\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Here, φ represents the walking direction in an extrinsic reference frame ( Fig. 5a ), β is the egocentric direction of the goal defined with respect to the axis of thrust, α is the visual angle between the FOE and the goal, w is a measure of the magnitude and angular area of flow due to environmental structure, v is observer velocity (which co-determines the flow rate), and k is a turning rate constant. Because β and α depend on φ , equation 1 is a well-defined dynamical system. Simulation results for the virtual heading error α are in Fig. 5b. With no flow ( w = 0), behavior is governed by the direction term, yielding a constant heading error of 10°. As the amount of optic flow increases ( w &gt; 0), the flow term comes to dominate, yielding smaller heading errors and faster turning rates, as observed in the human data (compare Fig. 3 , right column).\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The visual control law can thus accommodate the apparently contradictory empirical findings. When the flow is reduced or distorted, as on a grass lawn or at night, w is small, so behavior tends to be governed by egocentric direction. With greater flow and motion parallax, as in a savannah or forest, w increases, and optic flow dominates behavior. The visual system seems to depend on both variables in a complementary manner, thereby achieving\n",
      "\n",
      "Fig.  4. Mean  paths  and  virtual  heading  errors  in  experiment  2. ( a ) Computed offset control condition. ( b ) Prism condition. The four virtual worlds are the same as those in Fig. 3.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## articles\n",
      "\n",
      "Fig. 5. Visual control law for steering to a goal. ( a ) Definition of variables. Note that β = φ -ψ and α = ( φ + δ ) -ψ , so equation 1 is a welldefined dynamical system. ( b )  Simulations of equation 1. Each curve represents the time series of virtual heading error ( α ) for a constant value of w (0, 1, 3, 6), with k = 0.5 and δ = 10°. T o simulate the initiation of walking, v increased from 0 to 1 m/s as a logistic function of time over the first 2 s. Initial direction of walking was toward the goal, β = 0.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "robust locomotor control under a variety of environmental conditions. Contrary to previous claims 19-21 , humans indeed make use of optic flow to walk to a goal, and the specific flow variables exploited for locomotor control deserve further investigation 29 .\n",
      "\n",
      "## METHODS\n",
      "\n",
      "Subjects wore a ProView-80 HMD (Kaiser Electro-Optics, Carlsbad, California) covered with a black shield, so the surrounding field was dark. Stereo images of a three-dimensional virtual environment were generated on an Onyx2 graphics workstation (SGI, Mountain View, California) at 60 frames/s. Head position (4 mm RMS) and orientation (0.1° RMS) were measured with an IS-900 hybrid inertial/ultrasonic tracker (InterSense, Burlington, Massachusetts) and were used to update the view of the environment at 60 Hz, with a latency of 3 frames (50 ms). Head position records were subsequently downsampled at 30 Hz, filtered with a Butterworth filter (0.6 Hz cutoff), and ensemble averaged, to reduce the effects of gait oscillations. For analysis, records were divided into 15 segments, each 52 cm in length, and statistics were computed on the segment means. The first and last meter were not analyzed, to avoid initial transients and final lateral deviations that created large angular errors. Four virtual worlds were generated: first, the 'target line,' a vertical red line (8 mm diameter) that ran off the top and bottom of the display; second, the 'line + ground,' a red line that rested on a ground plane mapped with a black and white random noise texture; third, the 'doorway,' a 50-cm wide doorway on a frontal wall with a ground plane and ceiling having the noise texture; fourth, the 'doorway + posts,' an array of randomly positioned vertical blue posts (5-cm diameter) on both sides of the doorway, with approximately 19 posts visible in the first frame.\n",
      "\n",
      "Subjects were asked to walk to the goal. On each trial, the subject turned to face a marker and began walking when the virtual environment appeared. In experiment 1, the computed offset angle of the optic flow was chosen to be to the left ( δ = -10°) or right ( δ = 10°) randomly on each trial. T en volunteers performed eight trials in each 'world × offset' condition, blocked by world in a random order. In experiment 2, the subjects\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "wore a pair of 17.63 diopter wedge prisms in optometric frames inside the HMD. The computed offset angle was 0°, but the prisms displaced the image by 10° to the right; in the control condition, the prisms were not worn and the computed offset was 10°, always to the right. Twelve volunteers performed ten trials in each 'world × prism' condition, followed by three no-offset trials, blocked in a counterbalanced order. This research was approved by Brown's Institutional Review Board.\n",
      "\n",
      "## ACKNOWLEDGEMENTS\n",
      "\n",
      "The research was supported by the National Eye Institute (EY10923), National Institute of Mental Health (K02 MH01353) and the National Science Foundation (NSF 9720327). We thank A. Forsberg for his assistance, and T. Freeman for suggesting the second experiment.\n",
      "\n",
      "## RECEIVED 11 SEPTEMBER; ACCEPTED 4 DECEMBER 2000\n",
      "\n",
      "1. Gibson, J. J. Perception of the Visual World (Houghton Mifflin, Boston, 1950).\n",
      "2. Warren, W. H. in High-Level Motion Processing (ed. Watanabe, T.) 315-358 (MIT Press, Cambridge, Massachusetts, 1998).\n",
      "3. Warren, W. H., Morris, M. W. &amp; Kalish, M. Perception of translational heading from optical flow. J. Exp. Psychol. Hum. Percept. Perform. 14 , 646-660 (1988).\n",
      "4. van den Berg, A. V . Robustness of perception of heading from optic flow. Vision Res. 32 , 1285-1296 (1992).\n",
      "5. Crowell, J. A. &amp; Banks, M. S. Ideal observer for heading judgments. Vision Res. 36 , 471-490 (1996).\n",
      "6. Warren, W. H. &amp; Hannon, D. J. Direction of self-motion is perceived from optical flow. Nature 336 , 162-163 (1988).\n",
      "7. Royden, C. S., Banks, M. S. &amp; Crowell, J. A. The perception of heading during eye movements. Nature 360 , 583-585 (1992).\n",
      "8. Royden,  C.  S.,  Crowell,  J.  A.  &amp;  Banks,  M.  S.  Estimating  heading  during  eye movements. Vision Res. 34 , 3197-3214 (1994).\n",
      "9. Li, L. &amp; Warren, W. H. Perception of heading during rotation: sufficiency of dense motion parallax and reference objects. Vision Res. 40 , 3873-3894 (2000).\n",
      "10. Saito, H. et al. Integration of direction signals of image motion in the superior temporal sulcus of the macaque monkey. J. Neurosci. 6 , 145-157 (1986).\n",
      "11. Orban, G. A. et al. First-order analysis of optical flow in monkey brain. Proc. Natl. Acad. Sci. USA 89 , 2595-2599 (1992).\n",
      "12. Graziano, M. S. A., Andersen, R. A. &amp; Snowden, R. J. Tuning of MST neurons to spiral motions. J. Neurosci. 14 , 54-67 (1994).\n",
      "13. Duffy, C. J. &amp; Wurtz, R. H. Response of monkey MST neurons to optic flow stimuli with shifted centers of motion. J. Neurosci. 15 , 5192-5208 (1995).\n",
      "14. Siegel, R. M. &amp; Read, H. L. Analysis of optic flow in the monkey parietal area 7a. Cereb. Cortex 7 , 327-346 (1997).\n",
      "15. Anderson, K. C. &amp; Siegel, R. M. Optic flow selectivity in the anterior superior temporal  polysensory  area,  STPa,  of  the  behaving  monkey. J.  Neurosci. 19 , 2681-2692 (1999).\n",
      "16. DeJongh, B. M., Shipp, S., Skidmore, B., Frackowiak, R. S. J. &amp; Zeki, S. The cerebral activity related to the visual perception of forward motion in depth. Brain 117 , 1039-1054 (1994).\n",
      "17. Vaina, L. Complex motion perception and its deficits. Curr. Opin. Neurobiol. 8 , 494-502 (1998).\n",
      "18. Nakayama,  K.  James  Gibson-an  appreciation. Psychol.  Rev. 101 ,  329-335 (1994).\n",
      "19. Rushton, S. K., Harris, J. M., Lloyd, M. &amp; Wann, J. P . Guidance of locomotion on foot uses perceived target location rather than optic flow. Curr. Biol. 8 , 1191-1194 (1998).\n",
      "20. Harris, J. M. &amp; Rogers, B. J. Going against the flow. Trends Cogn. Sci. 3 , 449-450 (1999).\n",
      "21. Wann, J. &amp; Land, M. Steering with or without the flow: is the retrieval of heading necessary? Trends Cogn. Sci. 4 , 319-324 (2000).\n",
      "22. Collett, T. S. &amp; Land, M. F. Visual control of flight behavior in the hoverfly, Syritta pipiens . J. Comp. Physiol. 99 , 1-66 (1975).\n",
      "23. Wood, R. M., Harvey, M. A., Young, C. E., Beedie, A. &amp; Wilson, T. Weighting to go with the flow? Curr. Biol. 10 , R545-R546 (2000).\n",
      "24. Bridgeman, B., Kirch, M. &amp; Sperling, A. Segregation of cognitive and motor aspects  of  visual  function  using  induced  motion. Percept.  Psychophys. 29 , 336-342 (1981).\n",
      "25. Heckmann, T. &amp; Howard, I. P. Induced motion: isolation and dissociation of egocentric and vection-entrained components. Perception 20 , 285-305 (1991).\n",
      "26. Smeets, J. B. J. &amp; Brenner, E. Perception and action are based on the same visual information: distinction between position and velocity. J.  Exp.  Psychol.  Hum. Percept. Perform. 21 , 19-31 (1995).\n",
      "27. Schöner,  G.,  Dose,  M.  &amp;  Engels,  C.  Dynamics  of  behavior:  theory  and applications for autonomous robot architectures. Robot Auton. Syst. 16 , 213-245 (1995).\n",
      "28. Warren, W. H. Visually controlled locomotion: 40 years later. Eco. Psychol. 10 , 177-219 (1998).\n",
      "29. Fajen, B. &amp; Warren, W. H. Go with the flow. Trends Cogn. Sci. 4 , 368-369 (2000).\n"
     ]
    }
   ],
   "source": [
    "'''Basic conversion from .pdf -> DoclingDocument'''\n",
    "from docling.document_converter import DocumentConverter\n",
    "# extract text and tables as markdown\n",
    "conv_result = DocumentConverter().convert(DOC_SOURCE)\n",
    "print(result.document.export_to_markdown())\n",
    "# 0m52s for test document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61525af",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Exporting results to different formats'''\n",
    "import json\n",
    "# Export Docling document JSON format:\n",
    "with (output_dir / f\"{DOC_SOURCE}.json\").open(\"w+\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(json.dumps(conv_result.document.export_to_dict()))\n",
    "\n",
    "# Export Markdown format:\n",
    "with (output_dir / f\"{DOC_SOURCE}.md\").open(\"w+\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(conv_result.document.export_to_markdown())\n",
    "\n",
    "# Export Document Tags format:\n",
    "with (output_dir / f\"{DOC_SOURCE}.doctags\").open(\"w+\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(conv_result.document.export_to_doctags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "deef3a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-21 20:11:01,500 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-21 20:11:01,520 - INFO - Going to convert document batch...\n",
      "2025-12-21 20:11:01,522 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e784f57468f152386a904df3a8b24919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-21 20:11:01,573 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-21 20:11:01,576 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-21 20:11:01,579 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-21 20:11:01,753 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:11:01,755 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:11:01,928 [RapidOCR] download_file.py:60: File exists and is valid: /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:11:01,931 [RapidOCR] main.py:50: Using /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:11:02,406 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:11:02,408 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:11:02,414 [RapidOCR] download_file.py:60: File exists and is valid: /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:11:02,416 [RapidOCR] main.py:50: Using /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:11:02,634 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:11:02,636 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:11:02,808 [RapidOCR] download_file.py:60: File exists and is valid: /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:11:02,810 [RapidOCR] main.py:50: Using /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-21 20:11:03,429 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-21 20:11:03,432 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-21 20:11:04,415 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-21 20:11:05,517 - INFO - Processing document TestPDF.pdf\n",
      "\u001b[33m[WARNING] 2025-12-21 20:11:17,358 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2025-12-21 20:11:17,361 - WARNING - RapidOCR returned empty result!\n",
      "\u001b[33m[WARNING] 2025-12-21 20:11:32,500 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2025-12-21 20:11:32,504 - WARNING - RapidOCR returned empty result!\n",
      "\u001b[33m[WARNING] 2025-12-21 20:11:34,630 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2025-12-21 20:11:34,632 - WARNING - RapidOCR returned empty result!\n",
      "\u001b[33m[WARNING] 2025-12-21 20:11:37,104 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2025-12-21 20:11:37,106 - WARNING - RapidOCR returned empty result!\n",
      "2025-12-21 20:11:41,501 - WARNING - RapidOCR returned empty result!\n",
      "2025-12-21 20:12:01,977 - INFO - Finished converting document TestPDF.pdf in 60.49 sec.\n"
     ]
    }
   ],
   "source": [
    "'''Custom Conversion .pdf -> Docling Document'''\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.do_ocr = True\n",
    "pipeline_options.do_table_structure = True\n",
    "pipeline_options.generate_page_images = True\n",
    "pipeline_options.table_structure_options.do_cell_matching = True\n",
    "\n",
    "conv_result = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ").convert(DOC_SOURCE)\n",
    "# 0m52s for test document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b43f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from collections.abc import Iterable\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "from docling_core.types.doc import ImageRefMode\n",
    "\n",
    "from docling.backend.docling_parse_v4_backend import DoclingParseV4DocumentBackend\n",
    "from docling.datamodel.base_models import ConversionStatus, InputFormat\n",
    "from docling.datamodel.document import ConversionResult\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "def export_documents(\n",
    "    conv_results: Iterable[ConversionResult],\n",
    "    output_dir: Path,\n",
    "):\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    success_count = 0\n",
    "    failure_count = 0\n",
    "    partial_success_count = 0\n",
    "\n",
    "    for conv_res in conv_results:\n",
    "        if conv_res.status == ConversionStatus.SUCCESS:\n",
    "            success_count += 1\n",
    "            doc_filename = conv_res.input.file.stem\n",
    "\n",
    "            conv_res.document.save_as_markdown(\n",
    "                output_dir / f\"{doc_filename}.md\",\n",
    "                image_mode=ImageRefMode.PLACEHOLDER,\n",
    "            )\n",
    "\n",
    "            # Export Docling document format to markdown:\n",
    "            with (output_dir / f\"{doc_filename}.md\").open(\"w\") as fp:\n",
    "                fp.write(conv_res.document.export_to_markdown())\n",
    "\n",
    "        elif conv_res.status == ConversionStatus.PARTIAL_SUCCESS:\n",
    "            _log.info(\n",
    "                f\"Document {conv_res.input.file} was partially converted with the following errors:\"\n",
    "            )\n",
    "            for item in conv_res.errors:\n",
    "                _log.info(f\"\\t{item.error_message}\")\n",
    "            partial_success_count += 1\n",
    "        else:\n",
    "            _log.info(f\"Document {conv_res.input.file} failed to convert.\")\n",
    "            failure_count += 1\n",
    "\n",
    "    _log.info(\n",
    "        f\"Processed {success_count + partial_success_count + failure_count} docs, \"\n",
    "        f\"of which {failure_count} failed \"\n",
    "        f\"and {partial_success_count} were partially converted.\"\n",
    "    )\n",
    "    return success_count, partial_success_count, failure_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc20502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-21 20:26:43,612 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-21 20:26:43,835 - INFO - Going to convert document batch...\n",
      "2025-12-21 20:26:43,838 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e784f57468f152386a904df3a8b24919\n",
      "2025-12-21 20:26:43,844 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-21 20:26:43,847 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-21 20:26:43,850 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-21 20:26:43,938 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:26:43,940 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:26:44,086 [RapidOCR] download_file.py:60: File exists and is valid: /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:26:44,088 [RapidOCR] main.py:50: Using /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:26:44,997 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:26:45,001 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:26:45,018 [RapidOCR] download_file.py:60: File exists and is valid: /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:26:45,022 [RapidOCR] main.py:50: Using /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:26:45,262 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:26:45,264 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:26:45,531 [RapidOCR] download_file.py:60: File exists and is valid: /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-21 20:26:45,533 [RapidOCR] main.py:50: Using /home/mkp/python_code/data-cleaning/.venv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-21 20:26:46,382 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-21 20:26:46,386 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-21 20:26:47,796 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-21 20:26:51,556 - INFO - Processing document TestPDF.pdf\n",
      "\u001b[33m[WARNING] 2025-12-21 20:27:05,372 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2025-12-21 20:27:05,376 - WARNING - RapidOCR returned empty result!\n",
      "\u001b[33m[WARNING] 2025-12-21 20:27:20,676 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2025-12-21 20:27:20,677 - WARNING - RapidOCR returned empty result!\n",
      "\u001b[33m[WARNING] 2025-12-21 20:27:22,899 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2025-12-21 20:27:22,900 - WARNING - RapidOCR returned empty result!\n",
      "\u001b[33m[WARNING] 2025-12-21 20:27:25,251 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2025-12-21 20:27:25,254 - WARNING - RapidOCR returned empty result!\n",
      "2025-12-21 20:27:27,995 - WARNING - RapidOCR returned empty result!\n",
      "2025-12-21 20:27:42,459 - INFO - Finished converting document TestPDF.pdf in 58.85 sec.\n",
      "2025-12-21 20:27:42,565 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-21 20:27:42,597 - INFO - Going to convert document batch...\n",
      "2025-12-21 20:27:42,599 - INFO - Processing document ResearchPaper.pdf\n"
     ]
    }
   ],
   "source": [
    "# Location of sample PDFs used by this example. If your checkout does not\n",
    "# include test data, change `data_folder` or point `input_doc_paths` to\n",
    "# your own files.\n",
    "data_folder = Path(\"data\")\n",
    "input_doc_paths = [\n",
    "    data_folder / \"TestPDF.pdf\",\n",
    "    data_folder / \"ResearchPaper.pdf\"\n",
    "]\n",
    "\n",
    "# Configure the PDF pipeline. Enabling page image generation improves HTML\n",
    "# previews (embedded images) but adds processing time.\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.generate_page_images = True\n",
    "\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options, backend=DoclingParseV4DocumentBackend\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Convert all inputs. Set `raises_on_error=False` to keep processing other\n",
    "# files even if one fails; errors are summarized after the run.\n",
    "conv_results = doc_converter.convert_all(\n",
    "    input_doc_paths,\n",
    "    raises_on_error=False,  # to let conversion run through all and examine results at the end\n",
    ")\n",
    "# Write outputs to ./scratch and log a summary.\n",
    "_success_count, _partial_success_count, failure_count = export_documents(\n",
    "    conv_results, output_dir=Path(\"scratch\")\n",
    ")\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "print(f\"Document conversion complete in {end_time:.2f} seconds.\")\n",
    "\n",
    "if failure_count > 0:\n",
    "    raise RuntimeError(\n",
    "        f\"The example failed converting {failure_count} on {len(input_doc_paths)}.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03546f",
   "metadata": {},
   "source": [
    "## Text Processing & Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06402bd9",
   "metadata": {},
   "source": [
    "### Hybrid Chunker - [Official Docs](https://docling-project.github.io/docling/concepts/chunking/#hybrid-chunker)\n",
    "Hybrid chunking applies tokenization-aware refinements on top of document-based hierarchical chunking. This means it creates one chunk for each individual document element. It starts with the `HierarchicalChunker` and, using a user-provided tokenizern it completes two passes:\n",
    "1) Splits chunks only when needed.\n",
    "2) Merges chunks when possible (can turn this step off using `merge_peers=False` in constructor).\n",
    "The `HeirarchicalChunker` attaches relevant metadata like headers and captions by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e87ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# load in data\n",
    "doc = DocumentConverter().convert(source=DOC_SOURCE).document\n",
    "\n",
    "# create chunker\n",
    "chunker = HybridChunker()\n",
    "chunk_iter = chunker.chunk(dl_doc=doc)\n",
    "\n",
    "# usually want to embed context rich versions (includes metadata in each chunk)\n",
    "for i, chunk in enumerate(chunk_iter):\n",
    "    enriched_text = chunker.contextualize(chunk=chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
